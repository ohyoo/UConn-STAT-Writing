```{r tts-setup, include=FALSE}
tts_box <- function(text, id = paste0("abs", as.integer(as.numeric(Sys.time())))) {
  if (!knitr::is_html_output()) {
    return(htmltools::tagList(htmltools::tags$p(text)))
  }
  
  css <- "
  .abstract-tts { border:1px solid #e5e7eb; border-radius:8px; padding:.75rem 1rem; background:#fafafa; margin:.5rem 0 1rem; }
  .tts-controls { display:flex; align-items:center; gap:.5rem; margin-top:.5rem; flex-wrap:wrap; }
  .tts-controls button { padding:.35rem .6rem; border-radius:.5rem; border:1px solid #d1d5db; background:#fff; cursor:pointer; }
  .tts-controls button:disabled { opacity:.5; cursor:not-allowed; }"
  
  js <- "
  (function () {
    const synth = window.speechSynthesis;
    let currentToken = 0;          // increments on each (re)start
    let currentContainer = null;   // track which box is speaking

    function getContainer(el){ return el.closest('.abstract-tts'); }
    function getText(container){
      const clone = container.cloneNode(true);
      clone.querySelectorAll('.tts-controls').forEach(el=>el.remove());
      return (clone.textContent || '').replace(/\\s+/g,' ').trim();
    }
    function setState(container, state){
      container.dataset.state = state;
      const play  = container.querySelector('.tts-play');
      const pause = container.querySelector('.tts-pause');
      const stop  = container.querySelector('.tts-stop');
      if(!play || !pause || !stop) return;
      if(state==='idle'){
        play.disabled=false; pause.disabled=true; stop.disabled=true; play.textContent='▶︎ Play';
      }else if(state==='playing'){
        play.disabled=true; pause.disabled=false; stop.disabled=false; pause.textContent='⏸ Pause';
      }else{ // paused
        play.disabled=false; pause.disabled=false; stop.disabled=false; play.textContent='▶︎ Resume'; pause.textContent='▶︎ Pause';
      }
    }

    function speak(container){
      if(!synth){ alert('Speech Synthesis not supported in this browser.'); return; }
      // If resuming from paused, just resume
      if (synth.paused && currentContainer === container) {
        synth.resume(); setState(container,'playing'); return;
      }
      // Fresh start
      if (synth.speaking) synth.cancel();
      const token = ++currentToken;
      currentContainer = container;
      const rate = parseFloat(container.querySelector('.tts-rate')?.value || '1.0');
      const utter = new SpeechSynthesisUtterance(getText(container));
      utter.rate = rate;
      utter.onend = () => { if (token === currentToken) setState(container,'idle'); };
      utter.onerror = () => { if (token === currentToken) setState(container,'idle'); };
      synth.speak(utter);
      setState(container,'playing');
    }

    function pause(container){
      if(!synth || currentContainer !== container) return;
      if (synth.speaking && !synth.paused){ synth.pause(); setState(container,'paused'); }
      else if (synth.paused){ synth.resume(); setState(container,'playing'); }
    }

    function stop(container){
      if(!synth) return;
      if (synth.speaking || synth.paused){
        synth.cancel();
        currentToken++;            // invalidate any pending onend from prior utterances
        setState(container,'idle');
      }
    }

    function restartWithRate(container){
      if(!synth) return;
      const wasPaused = synth.paused && currentContainer === container;
      if (synth.speaking || synth.paused){ synth.cancel(); currentToken++; }
      // Defer to next tick so cancel fully flushes before speaking again
      setTimeout(() => {
        speak(container);
        if (wasPaused) { synth.pause(); setState(container,'paused'); }
      }, 0);
    }

    function initAll(){ document.querySelectorAll('.abstract-tts').forEach(c => setState(c,'idle')); }

    // Event delegation (robust with gitbook/bs4 dynamic loads)
    document.addEventListener('click', function(ev){
      const btn = ev.target.closest('.tts-play, .tts-pause, .tts-stop');
      if(!btn) return;
      const container = getContainer(btn);
      if(!container) return;
      ev.preventDefault();
      if (btn.classList.contains('tts-play'))  speak(container);
      if (btn.classList.contains('tts-pause')) pause(container);
      if (btn.classList.contains('tts-stop'))  stop(container);
    });

    // Use 'change' (not 'input') to avoid rapid restarts while dragging
    document.addEventListener('change', function(ev){
      if (!ev.target.matches('.tts-rate')) return;
      const container = getContainer(ev.target);
      if(!container) return;
      if (synth && (synth.speaking || synth.paused)) restartWithRate(container);
      // If idle, the new rate will apply on next Play automatically
    });

    if (document.readyState === 'loading') {
      document.addEventListener('DOMContentLoaded', initAll);
    } else { initAll(); }
  })();
  "
  
  htmltools::tagList(
    htmltools::tags$style(css),
    htmltools::tags$div(
      class="abstract-tts", id=id,
      htmltools::tags$p(text),
      htmltools::tags$div(
        class="tts-controls", `aria-label`="Abstract audio controls",
        htmltools::tags$button(class="tts-play",  type="button", "▶︎ Play"),
        htmltools::tags$button(class="tts-pause", type="button", disabled=NA, "⏸ Pause"),
        htmltools::tags$button(class="tts-stop",  type="button", disabled=NA, "⏹ Stop"),
        htmltools::tags$label(
          style="margin-left:.5rem;",
          "Speed ",
          htmltools::tags$input(class="tts-rate", type="range", min="0.6", max="1.6", step="0.1", value="1.0")
        )
      )
    ),
    htmltools::tags$script(js)
  )
}
```

# Writing Specific Sections

## Title

The title is the most read part of an article, and influences whether a reader 
is interested in reading the manuscript.

@hairston2003successful suggest that the title of a research paper 
should accomplish four goals:

+ predict the content of the research paper;
+ be interesting to the reader;
+ reflect the tone of the writing;
+ contain important keywords (that makes it easier to be found from keywords
  search).
  
The title of a paper is usually determined when the paper is close to
completion. To come up with a good title, list some key phrases that you would
like to have, and be creative in forming a good title that consists of most of 
them. Here are some tips.

+ Be informative by including these aspects: topic, method(s), data, and
  results.
+ Consider adding a subtitle to give more specifics about the paper.
+ Use appropriate critical keywords to increase the discoverability of the
  paper.
+ Follow the requirements from the instructions or journals.
+ Keep it as concise as possible.

:::{.example}
**Less effective vs. effective titles**

1. *A Comprehensive Regression Shrinkage Estimation and Variable Selection Procedure Using a Novel Least Absolute Shrinkage and Selection Operator*  
+ Too long, technical, and redundant; buries the key idea.  

<details>
<summary>Reveal the real title</summary>

*Regression Shrinkage and Selection via the Lasso* [@tibshirani1996lasso]  

  + Concise and memorable; immediately signals novelty.  
</details>

---

2. *A New Nonparametric Resampling-Based Monte Carlo Estimator for Determining the Optimal Quantity of Groupings with Multivariate Data*  
+ Overly technical; hard to parse at a glance.  

<details>
<summary>Reveal the real title</summary>

*Estimating the Number of Clusters in a Data Set via the Gap Statistic* [@tibshirani2001gap] 

  + States problem, method, and context clearly.  
</details>

---

3. *A Stepwise Algorithm for Sequentially Updating Correlation-Adjusted Regression Coefficients in High-Dimensional Settings with Strong Multicollinearity*  
+ Too wordy and technical; makes the method sound less accessible.  

<details>
<summary>Reveal the real title</summary>

*Least Angle Regression* [@efron2004lars]  

  + Short, clear, and intriguing; signals both novelty and relevance.  
</details>

---

4. *A Generalized Ensemble Learning Framework Based on Aggregating Decision Tree Classifiers for Improved Prediction Accuracy with Complex Data*  
+ Technically accurate but long, clunky, and forgettable.  

<details>
<summary>Reveal the real title</summary>

*Random Forests* [@breiman2001rf]  

  + Extremely concise and memorable; easy to cite and recall.  
</details>

:::



::: {.exercise}
**Can you guess the title from the abstract?**

1. Abstract: 

```{r, echo=FALSE, results='asis'}
tts_box("The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-
to-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.0 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature.")
```


<details>
<summary>Reveal the real title</summary>

*Attention Is All You Need* [@vaswani2017attention]
  
  + Strikingly simple and memorable; it reframed the field with just five words.  
</details>

---

2. Abstract:

```{r, echo=FALSE, results='asis'}
tts_box("We propose an adaptive nuclear norm penalization approach for low-rank matrix approximation, and use it to develop a new reduced rank estimation method for high-dimensional multivariate regression. The adaptive nuclear norm is defined as the weighted sum of the singular values of the matrix, and it is generally nonconvex under the natural restriction that the weight decreases with the singular value. However, we show that the proposed nonconvex penalized regression method has a global optimal solution obtained from an adaptively soft-thresholded singular value decomposition. The method is computationally efficient, and the resulting solution path is continuous. The rank consistency of and prediction/estimation performance bounds for the estimator are established for a high-dimensional asymptotic regime. Simulation studies and an application in genetics demonstrate its efficacy.")
```

<details>
<summary>Reveal the real title</summary>

*Reduced Rank Regression via Adaptive Nuclear Norm Penalization* [@chen2013rrr]

  + Highlights both the framework and the new contribution.  
</details>

---

3. Abstract:

```{r, echo=FALSE, results='asis'}
tts_box("We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.")
```


<details>
<summary>Reveal the real title</summary>

*Adam: A Method for Stochastic Optimization* [@kingma2015adam] 

  + Minimalist and branded: a single word, “Adam,” now instantly recognized in ML.  
</details>


:::







## Abstract

@marron1999effective recommended the following:

> Abstract material needs to be carefully chosen. A balance between the
> twin goals of brevity and maximal information content should again be
> carefully sought. There is room for more detail than in the title, but not
> enough room for all ideas covered in the paper. Make sure each "high point"
> is included. The paper will have a better chance in the review process if it
> is made clear what is done, and why it is important, since this will
> immediately capture the interest of the reviewer.

> Any recommendations for length here must be more case dependent. Longer
> papers will usually need longer abstracts. However, something between 4
> and 10 sentences is reasonable for most situations.

> Mathematical notation is rarely useful in the abstract. Sometimes notation is
> introduced in an abstract, and then not used at all! Even when notation is
> used in the abstract, the point can usually be conveyed more efficiently in
> words alone.

Tips:

+ Consider ppen with a sentence to establish the importance of the subject of the paper.
+ Identify a gap in the literature to set up the background and motivation of the paper.
+ Highlight the novelty/contributions of the paper.
+ For application papers, allude to new discoveries and their impacts.
+ For method papers, outline the essence of the methodology, and evidence from
  theoretical and numerical studies supporting the methods.
+ It must make sense when read in isolation for those who only read the 
  abstract, and must also provide a clear and accurate summary of the manuscript
  for readers who read the entire manuscript [@zeiger2000writing].  
+ Should not include citations.



### Compoments in an Abstract

There are five major components in an abstract: 

- **Context** (1–2 sentences): situate the problem and why it matters.
- **Objective** (1 sentence): the question or aim.
- **Approach** (1–2 sentences): data, design, and key methods.
- **Findings** (1–2 sentences): the most important results.
- **Implications** (1 sentence): what the results imply (perhaps in a broader context).

Components can also blend. For example, the objective and approach may appear together in one line, e.g., ‘We aim to estimate Y by developing X’, and a single sentence may introduce a novel approach and immediately state the key result, e.g., ‘By introducing A, we show B’.

There are other optional components (sometimes required by the venue):

- Data set: sample size or cohorts (applied/clinical).
- Assumptions/Limitations: stating key conditions or the main caveat (theory/methods).
- Uncertainty/Effect size: CIs, rates, or precision that quantify findings.
- Software/Computation/Availability: runtime/complexity; package; code/data source.
- Registration/Ethics: trial ID, preregistration, or IRB when human subjects are involved.
- Funding/Disclosure: only if the outlet asks for it in the abstract.

> **Length:** typecially ~150–250 words for journals. 


---

::: {.example}
Transfer learning under large-scale low-rank regression models (A recent JASA paper)

<!-- Legend -->
<span style="background:#dbeafe;padding:.1em .3em;border-radius:.2em">Context</span> 
<span style="background:#e9d5ff;padding:.1em .3em;border-radius:.2em">Objective</span> 
<span style="background:#dcfce7;padding:.1em .3em;border-radius:.2em">Approach</span> 
<span style="background:#ffedd5;padding:.1em .3em;border-radius:.2em">Findings</span> 
<span style="background:#fee2e2;padding:.1em .3em;border-radius:.2em">Implications</span>

<!-- Color-coded abstract -->
<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">In high-dimensional multiple response regression problems, the large dimensionality of the coefficient matrix poses a challenge to parameter estimation.</span> 
<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">To address this challenge, low-rank matrix estimation methods have been developed to facilitate parameter estimation in the high-dimensional regime, where the number of parameters increases with sample size.</span> 
<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Despite these methodological advances, accurately predicting multiple responses with limited target data remains a difficult task.</span> 
<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">To gain statistical power, the use of diverse datasets from source domains has emerged as a promising approach.</span> <span style="background:#e9d5ff;padding:.1em .2em;border-radius:.2em">In this paper, we focus on the problem of transfer learning in a high-dimensional multiple response regression framework, which aims to improve estimation accuracy by transferring knowledge from informative source datasets.</span> <span style="background:#dcfce7;padding:.1em .2em;border-radius:.2em">To reduce potential performance degradation due to the transfer of knowledge from irrelevant sources, we propose a novel transfer learning procedure including the forward selection of informative source sets.</span> <span style="background:#dcfce7;padding:.1em .2em;border-radius:.2em">In particular, our forward source selection method is new compared to existing transfer learning framework, offering deeper theoretical insights and substantial methodological innovations.</span> <span style="background:#dcfce7;padding:.1em .2em;border-radius:.2em">In addition, we develop an alternative transfer learning based on non-convex penalization to ensure rank consistency.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">Theoretical results show that the proposed estimator achieves a faster convergence rate than the single-task penalized estimator using only target data.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">Through simulations and real data experiments, we provide empirical evidence for the effectiveness of the proposed method and for its superiority over other methods.</span>

<!-- (Optional) Implications sentence—this abstract doesn’t currently include one -->
<span style="background:#fee2e2;padding:.1em .2em;border-radius:.2em"><em>(Optional) The proposed framework clarifies when and how multi-source transfer improves multi-response prediction, guiding principled use of external data in high-dimensional applications.</em></span>

:::


::: {.example}
Conformal prediction with conditional guarantees (A recent JRSSB paper)

<!-- Legend -->
<span style="background:#dbeafe;padding:.1em .3em;border-radius:.2em">Context</span> 
<span style="background:#e9d5ff;padding:.1em .3em;border-radius:.2em">Objective</span> 
<span style="background:#dcfce7;padding:.1em .3em;border-radius:.2em">Approach</span> 
<span style="background:#ffedd5;padding:.1em .3em;border-radius:.2em">Findings</span> 
<span style="background:#fee2e2;padding:.1em .3em;border-radius:.2em">Implications</span>

<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">We consider the problem of constructing distribution-free prediction sets with finite-sample conditional guarantees.</span>
<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Prior work has shown that it is impossible to provide exact conditional coverage universally in finite samples.</span>
<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Thus, most popular methods only guarantee marginal coverage over the covariates or are restricted to a limited set of conditional targets, e.g. coverage over a finite set of prespecified subgroups.</span> <span style="background:#e9d5ff;padding:.1em .2em;border-radius:.2em">This paper bridges this gap by defining a spectrum of problems that interpolate between marginal and conditional validity.</span> <span style="background:#dcfce7;padding:.1em .2em;border-radius:.2em">We motivate these problems by reformulating conditional coverage as coverage over a class of covariate shifts.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">When the target class of shifts is finite-dimensional, we show how to simultaneously obtain exact finite-sample coverage over all possible shifts.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">For example, given a collection of subgroups, our prediction sets guarantee coverage over each group.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">For more flexible, infinite-dimensional classes where exact coverage is impossible, we provide a procedure for quantifying the coverage errors of our algorithm.</span> <span style="background:#dcfce7;padding:.1em .2em;border-radius:.2em">Moreover, by tuning interpretable hyperparameters, we allow the practitioner to control the size of these errors across shifts of interest.</span> <span style="background:#fee2e2;padding:.1em .2em;border-radius:.2em">Our methods can be incorporated into existing split conformal inference pipelines, and thus can be used to quantify the uncertainty of modern black-box algorithms without distributional assumptions.</span>

:::


::: {.example}
Exact Bayesian inference for fitting stochastic epidemic models to partially observed incidence data (A recent AoAS paper)

<!-- Legend -->
<span style="background:#dbeafe;padding:.1em .3em;border-radius:.2em">Context</span> 
<span style="background:#e9d5ff;padding:.1em .3em;border-radius:.2em">Objective</span> 
<span style="background:#dcfce7;padding:.1em .3em;border-radius:.2em">Approach</span> 
<span style="background:#ffedd5;padding:.1em .3em;border-radius:.2em">Findings</span> 
<span style="background:#fee2e2;padding:.1em .3em;border-radius:.2em">Implications</span>


<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Stochastic epidemic models provide an interpretable probabilistic description of the spread of a disease through a population.</span>
<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Yet fitting these models to partially observed data can be a difficult task due to intractability of the marginal likelihood, even for classic Markovian models.</span> <span style="background:#e9d5ff;padding:.1em .2em;border-radius:.2em">To remedy this issue, this article introduces a novel data-augmented Markov chain Monte Carlo sampler for exact Bayesian inference under the stochastic susceptible-infectious-removed model, given only discretely observed counts of infections.</span> <span style="background:#dcfce7;padding:.1em .2em;border-radius:.2em">In a Metropolis–Hastings step, the latent data are jointly proposed from a surrogate process carefully designed to closely resemble the target process and from which we can efficiently generate epidemics consistent with the observed data.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">This yields a method that explores the high-dimensional latent space efficiently and easily scales to outbreaks with thousands of infections.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">We prove that our sampler is uniformly ergodic and find empirically that it mixes much faster than existing single-site samplers.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">We apply the algorithm to fit a semi-Markov susceptible-infectious-removed model to the 2013–2015 outbreak of Ebola Haemorrhagic Fever in Guéckédou, Guinea.</span>

<!-- No explicit implications sentence present -->
<span style="background:#fee2e2;padding:.1em .2em;border-radius:.2em"><em>(Optional) Implication: The sampler enables principled Bayesian inference for partially observed epidemics at realistic scales, improving reliability of outbreak analysis and forecasting.</em></span>

:::

::: {.example}
Statistical significance of clustering for count data (A recent Biometrics paper)

<!-- Legend -->
<span style="background:#dbeafe;padding:.1em .3em;border-radius:.2em">Context</span> 
<span style="background:#e9d5ff;padding:.1em .3em;border-radius:.2em">Objective</span> 
<span style="background:#dcfce7;padding:.1em .3em;border-radius:.2em">Approach</span> 
<span style="background:#ffedd5;padding:.1em .3em;border-radius:.2em">Findings</span> 
<span style="background:#fee2e2;padding:.1em .3em;border-radius:.2em">Implications</span>


<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Clustering is widely used in biomedical research for meaningful subgroup identification.</span> <span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">However, most existing clustering algorithms do not account for the statistical uncertainty of the resulting clusters and consequently may generate spurious clusters due to natural sampling variation.</span> <span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">To address this problem, the Statistical Significance of Clustering (SigClust) method was developed to evaluate the significance of clusters in high-dimensional data.</span>
<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">While SigClust has been successful in assessing clustering significance for continuous data, it is not specifically designed for discrete data, such as count data in genomics.</span> <span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Moreover, SigClust and its variations can suffer from reduced statistical power when applied to non-Gaussian high-dimensional data.</span> <span style="background:#e9d5ff;padding:.1em .2em;border-radius:.2em">To overcome these limitations, we propose SigClust-DEV, a method designed to evaluate the significance of clusters in count data.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">Through extensive simulations, we compare SigClust-DEV against other existing SigClust approaches across various count distributions and demonstrate its superior performance.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">Furthermore, we apply our proposed SigClust-DEV to Hydra single-cell RNA sequencing (scRNA) data and electronic health records (EHRs) of cancer patients to identify meaningful latent cell types and patient subgroups, respectively.</span>

<!-- No explicit implications sentence present -->
<span style="background:#fee2e2;padding:.1em .2em;border-radius:.2em"><em>(Optional) Implication: Rigorous significance testing for clusters in count data can reduce false discoveries and sharpen biological and clinical subgrouping.</em></span>


:::




::: {.example}
Robust transfer learning with unreliable source data (A recent AoS paper)

<!-- Legend -->
<span style="background:#dbeafe;padding:.1em .3em;border-radius:.2em">Context</span> 
<span style="background:#e9d5ff;padding:.1em .3em;border-radius:.2em">Objective</span> 
<span style="background:#dcfce7;padding:.1em .3em;border-radius:.2em">Approach</span> 
<span style="background:#ffedd5;padding:.1em .3em;border-radius:.2em">Findings</span> 
<span style="background:#fee2e2;padding:.1em .3em;border-radius:.2em">Implications</span>

<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">This paper addresses challenges in robust transfer learning stemming from ambiguity in Bayes classifiers and weak transferable signals between the target and source distributions.</span> <span style="background:#e9d5ff;padding:.1em .2em;border-radius:.2em">We introduce a novel quantity called the “ambiguity level” that measures the discrepancy between the target and source regression functions, propose a simple transfer learning procedure, and establish a general theorem that shows how this new quantity is related to the transferability of learning in terms of risk improvements.</span> <span style="background:#dcfce7;padding:.1em .2em;border-radius:.2em">Our proposed “Transfer Around Boundary” (TAB) method, with a threshold that balances the performance contributions of the target and source data, is shown to be both efficient and robust, improving classification while avoiding negative transfer.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">Moreover, we demonstrate the effectiveness of the TAB model on nonparametric classification and logistic regression tasks, achieving upper bounds which are optimal up to logarithmic factors.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">Simulation studies lend further support to the effectiveness of TAB.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">We also provide simple approaches to bound the excess misclassification error without the need for specialized knowledge in transfer learning.</span>

<!-- Optional implication to complete the five-part structure -->
<span style="background:#fee2e2;padding:.1em .2em;border-radius:.2em"><em>(Optional) Implication: By quantifying “ambiguity” and adapting transfer near class boundaries, TAB offers a principled recipe for robust performance gains while guarding against negative transfer in practice.</em></span>

:::




::: {.exercise}
Spot the Components of an Abstract

<span style="background:#dbeafe;padding:.1em .3em;border-radius:.2em">Context</span> 
<span style="background:#e9d5ff;padding:.1em .3em;border-radius:.2em">Objective</span> 
<span style="background:#dcfce7;padding:.1em .3em;border-radius:.2em">Approach</span> 
<span style="background:#ffedd5;padding:.1em .3em;border-radius:.2em">Findings</span> 
<span style="background:#fee2e2;padding:.1em .3em;border-radius:.2em">Implications</span>

**Abstract:**  
We consider the problem of constructing distribution-free prediction sets with finite-sample conditional guarantees. Prior work has shown that it is impossible to provide exact conditional coverage universally in finite samples. Thus, most popular methods only guarantee marginal coverage over the covariates or are restricted to a limited set of conditional targets, e.g. coverage over a finite set of prespecified subgroups. This paper bridges this gap by defining a spectrum of problems that interpolate between marginal and conditional validity. We motivate these problems by reformulating conditional coverage as coverage over a class of covariate shifts. When the target class of shifts is finite-dimensional, we show how to simultaneously obtain exact finite-sample coverage over all possible shifts. For example, given a collection of subgroups, our prediction sets guarantee coverage over each group. For more flexible, infinite-dimensional classes where exact coverage is impossible, we provide a procedure for quantifying the coverage errors of our algorithm. Moreover, by tuning interpretable hyperparameters, we allow the practitioner to control the size of these errors across shifts of interest. Our methods can be incorporated into existing split conformal inference pipelines, and thus can be used to quantify the uncertainty of modern black-box algorithms without distributional assumptions.

<details>
<summary><b>Show answer</b></summary>

<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">We consider the problem of constructing distribution-free prediction sets with finite-sample conditional guarantees.</span>
<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Prior work has shown that it is impossible to provide exact conditional coverage universally in finite samples.</span>
<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Thus, most popular methods only guarantee marginal coverage…</span> <span style="background:#e9d5ff;padding:.1em .2em;border-radius:.2em">This paper bridges this gap by defining a spectrum of problems that interpolate between marginal and conditional validity.</span> <span style="background:#dcfce7;padding:.1em .2em;border-radius:.2em">We motivate these problems by reformulating conditional coverage as coverage over a class of covariate shifts.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">When the target class of shifts is finite-dimensional… exact finite-sample coverage over all possible shifts.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">…given a collection of subgroups, our prediction sets guarantee coverage over each group.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">For more flexible, infinite-dimensional classes… we provide a procedure for quantifying the coverage errors…</span> <span style="background:#dcfce7;padding:.1em .2em;border-radius:.2em">Moreover, by tuning interpretable hyperparameters… control the size of these errors…</span> <span style="background:#fee2e2;padding:.1em .2em;border-radius:.2em">Our methods can be incorporated into existing split conformal pipelines… quantify the uncertainty of modern black-box algorithms without distributional assumptions.</span>
</details>

---

<span style="background:#dbeafe;padding:.1em .3em;border-radius:.2em">Context</span> 
<span style="background:#e9d5ff;padding:.1em .3em;border-radius:.2em">Objective</span> 
<span style="background:#dcfce7;padding:.1em .3em;border-radius:.2em">Approach</span> 
<span style="background:#ffedd5;padding:.1em .3em;border-radius:.2em">Findings</span> 
<span style="background:#fee2e2;padding:.1em .3em;border-radius:.2em">Implications</span>

**Abstract:**  
Stochastic epidemic models provide an interpretable probabilistic description of the spread of a disease through a population. Yet, fitting these models to partially observed data is a notoriously difficult task due to intractability of the likelihood for many classical models. To remedy this issue, this article introduces a novel data-augmented MCMC algorithm for exact Bayesian inference under the stochastic SIR model, given only discretely observed counts of infection. In a Metropolis-Hastings step, the latent data are jointly proposed from a surrogate process carefully designed to closely resemble the SIR model, from which we can efficiently generate epidemics consistent with the observed data. This yields a method that explores the high-dimensional latent space efficiently, and scales to outbreaks with hundreds of thousands of individuals. We show that the Markov chain underlying the algorithm is uniformly ergodic, and validate its performance via thorough simulation experiments and a case study on the 2013-2015 outbreak of Ebola Haemorrhagic Fever in Western Africa.

<details>
<summary><b>Show answer</b></summary>

<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Stochastic epidemic models provide an interpretable probabilistic description of the spread of a disease through a population.</span>
<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Yet, fitting these models to partially observed data is a notoriously difficult task due to intractability of the likelihood for many classical models.</span> <span style="background:#e9d5ff;padding:.1em .2em;border-radius:.2em">To remedy this issue, this article introduces a novel data-augmented MCMC algorithm for exact Bayesian inference under the stochastic SIR model, given only discretely observed counts of infection.</span> <span style="background:#dcfce7;padding:.1em .2em;border-radius:.2em">In a Metropolis-Hastings step, the latent data are jointly proposed from a surrogate process carefully designed to closely resemble the SIR model, from which we can efficiently generate epidemics consistent with the observed data.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">This yields a method that explores the high-dimensional latent space efficiently, and scales to outbreaks with hundreds of thousands of individuals.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">We show that the Markov chain underlying the algorithm is uniformly ergodic, and validate its performance via thorough simulation experiments and a case study on the 2013-2015 outbreak of Ebola Haemorrhagic Fever in Western Africa.</span>

<!-- No explicit implications sentence present -->
<span style="background:#fee2e2;padding:.1em .2em;border-radius:.2em"><em>(Optional) Implication: The algorithm enables principled, scalable Bayesian inference for partially observed epidemics at realistic population sizes.</em></span>
</details>


---


<span style="background:#dbeafe;padding:.1em .3em;border-radius:.2em">Context</span> 
<span style="background:#e9d5ff;padding:.1em .3em;border-radius:.2em">Objective</span> 
<span style="background:#dcfce7;padding:.1em .3em;border-radius:.2em">Approach</span> 
<span style="background:#ffedd5;padding:.1em .3em;border-radius:.2em">Findings</span> 
<span style="background:#fee2e2;padding:.1em .3em;border-radius:.2em">Implications</span>

**Abstract**:  
Statistical learning with a large number of rare binary features is commonly encountered in analyzing electronic health records (EHR) data, especially in the modeling of disease onset with prior medical diagnoses and procedures. Dealing with the resulting highly sparse and large-scale binary feature matrix is notoriously challenging as conventional methods may suffer from a lack of power in testing and inconsistency in model fitting, while machine learning methods may suffer from the inability of producing interpretable results or clinically-meaningful risk factors. To improve EHR-based modeling and use the natural hierarchical structure of disease classification, we propose a tree-guided feature selection and logic aggregation approach for large-scale regression with rare binary features, in which dimension reduction is achieved through not only a sparsity pursuit but also an aggregation promoter with the logic operator of “or”. We convert the combinatorial problem into a convex linearly-constrained regularized estimation, which enables scalable computation with theoretical guarantees. In a suicide risk study with EHR data, our approach is able to select and aggregate prior mental health diagnoses as guided by the diagnosis hierarchy of the International Classification of Diseases. By balancing the rarity and specificity of the EHR diagnosis records, our strategy improves both prediction and interpretation. We identify important higher-level categories and subcategories of mental health conditions and simultaneously determine the level of specificity needed for each of them in associating with suicide risk.

<details>
<summary><b>Show answer</b></summary>

<span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Statistical learning with a large number of rare binary features is commonly encountered in analyzing EHR data, especially in modeling disease onset with prior diagnoses and procedures.</span> <span style="background:#dbeafe;padding:.1em .2em;border-radius:.2em">Dealing with the resulting highly sparse and large-scale binary feature matrix is challenging… conventional methods lack power/consistency; ML methods lack interpretability/clinically-meaningful factors.</span> <span style="background:#e9d5ff;padding:.1em .2em;border-radius:.2em">To improve EHR-based modeling and leverage the disease hierarchy, we propose a tree-guided feature selection and logic aggregation approach for large-scale regression with rare binary features…</span> <span style="background:#dcfce7;padding:.1em .2em;border-radius:.2em">We convert the combinatorial problem into a convex linearly-constrained regularized estimation, enabling scalable computation with theoretical guarantees.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">In a suicide risk study, the method selects and aggregates diagnoses guided by ICD hierarchy.</span>
<span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">By balancing rarity and specificity, the strategy improves both prediction and interpretation.</span> <span style="background:#ffedd5;padding:.1em .2em;border-radius:.2em">It identifies important higher-level categories and subcategories of mental health conditions and determines the needed specificity for associating with suicide risk.</span> 

</details>

:::




::: {.exercise}
Compress an Abstract to the Required Length

**Task.** Read the original abstract (220 words), then rewrite it to ≤150 words, and then to ≤100 words while preserving the five core components: Context, Objective, Approach, Findings, Implications.

**Original (≈220 words)**  
Rare-event risk prediction from multi-site EHR data is hampered by extreme class imbalance and heterogeneity across institutions. Models that perform well at one site often fail to generalize, and naive pooling can hide site-specific signals that matter clinically. We develop **RankFuse**, a federated learning framework that consolidates rankings from related event phenotypes across sites without sharing raw data. Our objective is to improve identification of the highest-risk patients under strict privacy constraints. RankFuse uses truncated listwise losses with top-K–focused mini-batches and a cross-site consensus penalty that aligns phenotype-specific orderings while allowing site-level deviations. We derive conditions under which RankFuse improves precision at fixed review capacity and prove a finite-sample bound on recall@K under prevalence 0.2–1%. In simulations with controlled heterogeneity, RankFuse raises precision@1% by 16–24% over calibrated XGBoost and by 11–18% over focal-loss baselines. In a 10-hospital suicide cohort (N=850,000; 2,970 events), RankFuse improves precision@1% from 0.29 to 0.36 and recall@1% from 0.51 to 0.62, while preserving site-level calibration. These gains translate to earlier identification of high-risk patients without expanding the clinical review queue. The method fits into standard federated pipelines, requires only gradient sharing, and supports secure aggregation. Code for RankFuse and synthetic benchmarks will be released to facilitate adoption.

---

<details>
<summary><b>Sample ≤150-word version (≈148 words)</b></summary>

Rare-event risk prediction in multi-site EHRs suffers from class imbalance and cross-site heterogeneity, limiting generalization. We propose **RankFuse**, a privacy-preserving federated framework that consolidates phenotype-specific rankings across hospitals to better surface the highest-risk patients. RankFuse optimizes truncated listwise losses with top-K–focused mini-batches and a consensus penalty that aligns site rankings while allowing local deviations. We show conditions for improved precision at fixed capacity and provide a finite-sample bound on recall@K under 0.2–1% prevalence. In simulations with controlled heterogeneity, RankFuse raises precision@1% by 16–24% over calibrated XGBoost and by 11–18% over focal-loss baselines. In a 10-hospital suicide cohort (N=850,000; 2,970 events), it improves precision@1% from 0.29→0.36 and recall@1% from 0.51→0.62, maintaining calibration. These gains enable earlier identification without expanding review queues. The approach fits standard federated workflows with gradient sharing and secure aggregation; code and synthetic benchmarks will be released.
</details>

---

<details>
<summary><b>Sample ≤100-word version (≈98 words)</b></summary>

We introduce **RankFuse**, a federated ranking method for rare-event prediction in multi-site EHRs. Using truncated listwise losses and a cross-site consensus penalty, RankFuse aligns phenotype-specific rankings while preserving site idiosyncrasies. Theory gives conditions for precision gains at fixed capacity and a finite-sample bound on recall@K under 0.2–1% prevalence. Simulations show +16–24% precision@1% over calibrated XGBoost and +11–18% over focal-loss baselines. In a 10-hospital suicide cohort (N=850k; 2,970 events), precision@1% improves 0.29→0.36 and recall@1% 0.51→0.62 with calibration intact. RankFuse integrates into federated workflows via gradient sharing; code and synthetic benchmarks will be released.
</details>

---

**Discussion**

- What was removed or condensed in each shorter version, and why didn’t meaning suffer?
- Did all five components survive the compression? 

:::


## Keywords
Keywords are words in addition to those in the title that attract search
queries. Including the most relevant keywords helps other researchers find
your paper.


+ No need to repeat anything in the title already. 
+ List them in alphabetical order.
+ Contain words and phrases that suggest what the topic is about. 

## Introduction

The introduction section is always the first section of a paper. Some journals
may not call it introduction but require a section that serves the same
purpose. The purpose of the introduction is to stimulate the reader’s interest 
and to provide background information which is pertinent to the study 
[@jenkins1995write]. The introduction section guides the readers from a general 
subject 
area to the narrow topic of the paper. It should answer three questions:

+ Why does it matter?
+ What has already been done?
+ What is new?

That is, the introduction sections need to explain the importance of the topic 
of
the paper, provide the background of the research work, and highlight the
contributions of the work. At the end of the introduction, a roadmap, or an
outline of the paper is useful in helping the readers navigate through the
following sections.

The introduction is typically outlined at the very beginning of the writing
process, but completed towards the end after the other sections have been 
written.   Do NOT wait to perform the literature review until 
last, however!  This should happen *before* the research is undertaken to
ensure you are not duplicating something that has already been done!

An introduction often contains the following items.

+ An overview of the topic. Start with a general overview of your topic and
  narrow it to the specific subject you are addressing. Then, mention
  questions or concerns you have about the case. Explain why they are important
  and why it needs to be addressed right now.
+ Existing works. The introduction is the place to review other conclusions on 
  your
  topic. The literature review should be thorough, including both old and recent
  works. It should show that you are aware of prior research. It also introduces
  past findings to those who might not have that expertise. 
+ A gap needs to be
  identified from the importance of the topic and the current status of the
  literature, which is the rationale for your work. Why are existing methods not
  sufficient?  What are elements of an attractive solution?
+ Contributions. This is a thesis statement, which summarizes the the
  contributions of your work to the existing literature, and answers the "what
  is new" question.
+ A roadmap. A brief summary of what each section does in the paper. This 
  concludes the introduction.
  

## Data

The data section should provide all the details that are relevant for the
research project.

+ Who collected the data (source)?
+ How was the data collected? Sampling frame? Sampling approach?
+ What period or range does the data cover?
+ Why does the data help answer the research question?
+ What exploratory analyses are done (descriptives, visualization, etc.)?


## Methods

Notation is much easier to digest if the reader first understands the main
idea at an intuitive level.

+ Establish notation.
+ What are the observed data?
+ What are the models?
+ What are the parameters to be estimated?
+ How are the point estimators obtained?
+ How are the uncertainty (standard errors) of the point estimators assessed?
+ How are the variances of the point estimators estimated?
+ How are the null distribution of the testing statistics established?
+ Clearly state the assumptions and claims of theoretical results.

## Simulation

ADEMP [@morris2019using]:

+ Aims
+ Data generating mechanism
+ Estimand/target of analysis
+ Methods
+ Performance measures

Coding and Execution

Analysis/Discussion: with tables and figures.  May be included in a Results 
section.

+ For each table/figure, write down the bullet points to convey to the readers.
+ Group the bullet points in blocks and put the blocks in a logical order.
+ Within each block, put the bullet points in the right logical order.
+ Some (shorter) blocks can be converted into proper paragraphs in the final 
  paper, while other (longer) blocks may remain in bullet form.

## Application

+ Report the statistical analyses in tables/figures.
+ When summarizing from tables/figures, paint the big picture, rather than
  reiterating all of the little details.
+ Discussions to link the analyses back to the substantive topic
  [@miller2015chicago]:
  
> Having presented the individual pieces of evidence, an investigator must
> summarize how that evidence, taken together, support the conclusoin of the
> investigation. Statisticians should explain how the statistical evidence answers
> the question posed at the beginning of the paper, following standard
> expository writing guidelines to writing an analytic essay.


## Discussion and Conclusion

+ A summary, again, of the contributions of the research. 
+ The research question posed as the `need' of the introduction must be
  answered here [@zeiger2000writing].
+ Limitations of the current study
+ Future directions.

## Appendix

+ Technical details (e.g., proofs, algorithms) that would otherwise break the
  flow of the main text.
+ Data source details.

## Acknowledgements

This section is optional, but could be used to acknowledge certain individuals
who have contributed to the research and/or success of the manuscript (e.g.
peer reviewers).  

In general, if the research upon which you are writing was funded, the funding
agency and funding mechanism are typically included here unless otherwise 
specified.  

## References

+ Every reference cited in the paper should appear here.
+ References not cited should not appear here.
+ All are automatically taken care of by BibTeX.
+ Styles are controlled by bib style (`.bst` file).

## Online Supplement

+ Computer code
+ Data
+ Additional simulation results

## General Tips

From @jenkins1995write:

> In order to maintain continuity between the key sections (introduction, 
> methods, results and discussion) it is helpful to consider the manuscript as
> telling a story. 
>
> The strong parts to the story-line are the introduction and the discussion, so
> the link between these sections must be clear.  

Devices such as paragraphing, headings, indentation, and enumeration
help the reader see the major points that you want to make.

As a rule of thumb, if you type a full page (double spaced) without
indenting for a new paragraph, you almost certainly have run one thought
into another and have missed an opportunity to differentiate your ideas.

Any tables and figures included in the manuscript must be mentioned (referenced)
within the main text.

If journal/instructions do not specify otherwise, tables and figures should be
placed near (ideally after) the related text, and on the top of the page. 

Use consistent notation throughout the manuscript, avoid defining any 
unnecessary notation, and avoid using the same notation to describe different
things (variables, indices, etc.).  

## General Tips: Use of English

It is relatively easy to read and understand English which is well written. 
As the quality of writing deteriorates, however, it becomes progressively more 
difficult for the reader to understand the author's intended meaning. 

An obvious problem occurs when the author fails to use properly constructed 
sentences. This can be easily corrected with revision and external review.

A much more dangerous problem occurs when the author unconsciously assumes that 
the reader is able to follow his/her unwritten train of thought.  This can 
also usually be caught with external review.

More tips on the use of English:

+ Use of tenses: Use present tense for most of the manuscript; use past tense 
only when describing events that occurred in the past (data collection, an 
experiment that has already been conducted, how simulations were designed).  
Be consistent with tense usage within sections, paragraphs, etc.

+ Do not use the word 'significant' other than in a statistical sense.

+ The word 'data' is often mistakenly treated as if it were a singular noun
instead of as a plural noun.  The word 'data' is, in fact, a plural noun (its 
singular form is 'datum'). 
